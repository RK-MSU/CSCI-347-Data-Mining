{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "fkaF2f-HV_DW"
            },
            "source": [
                "River Kelly\n",
                "\n",
                "Kyler Gappa\n",
                "\n",
                "CSCI-347\n",
                "\n",
                "Project 03: Dimensinality Reduction and Clustering"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "W74kGwFQV_DZ"
            },
            "source": [
                "This project may be completed individually or with group of up to size three. Turn in the code\n",
                "and written responses in both Brightspace and Gradescope.\n",
                "\n",
                "Choose a data set that you are interested in from the UCI Machine Learning Repository that has\n",
                "at least five numerical attributes, and that you believe may contain clusters. Only use the numerical\n",
                "attributes for this project. Note: if you are planning to complete the extra credit portion of this\n",
                "project, you will need to use a data set that has class labels (ground truth cluster labels), i.e., a\n",
                "classification data set, in order to compute the accuracy of the clustering. If you would like to use\n",
                "a data set from a different source, please discuss this with me."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "uR5w1P2rV_Da"
            },
            "source": [
                "# Part 1: Think about the data"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "MNsDwGiOA6_T"
            },
            "source": [
                "  This data is interesting because it takes a unique approach to language analysis. An understanding of writing could help us recognise and decipher currently unreadable text. There are six numerical and zero categorical attributes. The repository says that there are no missing attributes in the dataset so no extra techniques will be used unless missing values are found. we expect that there will be a few clusters due to a few different reasons. There is a chance for clusters for each number (i.e. all 4's will form a cluster) and a chance for clusters of every number from people that write similarly. If clusters exist then we can use the range of values that fit within the cluster to help identify similar letters in the future. We expect for there to be four to ten clusters as there a few numbers that may have similar shapes or may have consistent shapes that are mathmatically similar. We expect there to be some clusters that are differing sizes as there are some shapes that more numbers fall into and that should result in a larger cluster (i.e. 6, 8, 9 vs 1, 7)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "bin9rArIV_Db"
            },
            "source": [
                "# Part 2: Write Python code for clustering"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "bhNjKB87V_Dc"
            },
            "source": [
                "Write the following functions in Python. You may use scikit-learn or other packages to check\n",
                "the correctness of your implementation, but you may not use any existing clustering algorithm\n",
                "implementation in your code."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "T1XQbo1pV_Dc"
            },
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import random"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "D2t7UHR3V_Dd"
            },
            "source": [
                "## 1. (10 points) *k*-means Clustering Algorithm"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "KTpjn0QMV_De"
            },
            "source": [
                "A function that implements the $k$-means clustering\n",
                "algorithm. The function should take a data matrix, a number of clusters $k$,\n",
                "and a convergence parameter $\\epsilon$, as input, and return the\n",
                "representatives (means) as well as the clusters found using $k$-means. If\n",
                "the distance is the same between a point and more than one representative\n",
                "(mean), then assign the point to the mean corresponding to the cluster with\n",
                "the lowest index."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "Ufy1c-iFV_Df"
            },
            "outputs": [],
            "source": [
                "def kMeanClustering(D, k_cluster: int, convergence = 0.0):\n",
                "    # var: points_dict\n",
                "    # --------------------------------------------------------------------------\n",
                "    # disctionary of points (x, y) from the data 'D'\n",
                "    points_dict = dict()\n",
                "\n",
                "    # populate points_dict\n",
                "    for index, point_data in enumerate(D):\n",
                "        points_dict[index] = (point_data[0], point_data[1])\n",
                "    # sort the points_dict\n",
                "    points_dict = {k: v for k, v in sorted(points_dict.items(), key=lambda item: item[1][1])}\n",
                "    \n",
                "\n",
                "    clusters = dict() # dictionary of clusters\n",
                "    \n",
                "    # init_points_indices\n",
                "    # -------------------\n",
                "    # list to help ensure that each cluster has a unique randomly selected point\n",
                "    init_points_indices = list() \n",
                "    # init clusters with random points\n",
                "    for i in range(k_cluster):\n",
                "        init_point_index = random.randint(0, D.shape[0]-1)\n",
                "        while init_point_index in init_points_indices:\n",
                "            init_point_index = random.randint(0, D.shape[0]-1)\n",
                "        init_points_indices.append(init_point_index)\n",
                "        init_point = points_dict[init_point_index]\n",
                "        # make cluster item\n",
                "        clusters[i] = {\n",
                "            'mean': (init_point[0], init_point[1]),\n",
                "            'points_indices': list()\n",
                "        }\n",
                "\n",
                "    # return\n",
                "    cluster_iters = list()\n",
                "    count_iter = 0\n",
                "    while True:\n",
                "        count_iter += 1\n",
                "        if count_iter > 10000:\n",
                "            break\n",
                "        # add points to clusters\n",
                "        for (point_index, point) in points_dict.items():\n",
                "            cluster_to_point_distances_dict = dict()\n",
                "            for (cluster_index, cluster) in clusters.items():\n",
                "                distance_x = cluster['mean'][0] - point[0]\n",
                "                distance_y = cluster['mean'][1] - point[1]\n",
                "                distance = ( (distance_x ** 2) + (distance_y ** 2) ) ** (1/2)\n",
                "                cluster_to_point_distances_dict[cluster_index] = distance\n",
                "            # sort\n",
                "            cluster_to_point_distances_dict = {k: v for k, v in sorted(cluster_to_point_distances_dict.items(), key=lambda item: item[1])}\n",
                "            closest_cluster_index = list(cluster_to_point_distances_dict.keys())[0]\n",
                "\n",
                "            # add point to closest cluser\n",
                "            clusters[closest_cluster_index]['points_indices'].append(point_index)\n",
                "            # update cluster mean\n",
                "            x_sum = y_sum = 0\n",
                "            for cluster_point_index in clusters[closest_cluster_index]['points_indices']:\n",
                "                cluster_point = points_dict[cluster_point_index]\n",
                "                x_sum += cluster_point[0]\n",
                "                y_sum += cluster_point[1]\n",
                "            num_cluster_points = len(clusters[closest_cluster_index]['points_indices'])\n",
                "            x_mean = x_sum / num_cluster_points\n",
                "            y_mean = y_sum / num_cluster_points\n",
                "            clusters[closest_cluster_index]['mean'] = (x_mean, y_mean)\n",
                "\n",
                "\n",
                "        total_distance_sum = 0\n",
                "        for (cluster_index, cluster) in clusters.items():\n",
                "            distance = 0\n",
                "            for point_index in cluster['points_indices']:\n",
                "                point = points_dict[point_index]\n",
                "                distance += (((point[0]-cluster['mean'][0])**2)+((point[1]-cluster['mean'][1])**2))**(1/2)\n",
                "            clusters[cluster_index]['dist_sum'] = distance\n",
                "            total_distance_sum += distance\n",
                "\n",
                "        cluster_iters.append({\n",
                "            'clusters': clusters,\n",
                "            'total_distance': total_distance_sum\n",
                "        })\n",
                "\n",
                "        if len(cluster_iters) > 1:\n",
                "            last_cluster_dist = cluster_iters[-2]['total_distance']\n",
                "            dist_diff = abs(total_distance_sum - last_cluster_dist)\n",
                "            if dist_diff <= convergence:\n",
                "                break\n",
                "\n",
                "        # remove cluster points for next iteration\n",
                "        new_clusters = dict()\n",
                "        for (i, cluster) in clusters.items():\n",
                "            new_clusters[i] = {\n",
                "                'mean': (cluster['mean'][0], cluster['mean'][1]),\n",
                "                'points_indices': list()\n",
                "            }\n",
                "        clusters = new_clusters\n",
                "\n",
                "    clusters = cluster_iters[-1]['clusters']\n",
                "    # print(cluster_iters[-1]['total_distance'])\n",
                "\n",
                "\n",
                "    pred_labels = [0] * D.shape[0]\n",
                "    for cluster_index in clusters:\n",
                "        cluster = clusters[cluster_index]\n",
                "        for point in cluster['points_indices']:\n",
                "            pred_labels[point] = cluster_index\n",
                "    \n",
                "    centers = np.ndarray(shape=(k_cluster, 2))\n",
                "    for i, cluster in enumerate(clusters.values()):\n",
                "        centers[i] = cluster['mean']\n",
                "\n",
                "    plt.scatter(D[:,0], D[:,1], c=pred_labels)\n",
                "    plt.scatter(centers[:,0], centers[:,1], c='red')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "zu4m_k2IRwxQ"
            },
            "outputs": [],
            "source": [
                "from sklearn.datasets import make_blobs\n",
                "D, labels = make_blobs(n_samples=100, centers=3, cluster_std=.3, random_state=0)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 312
                },
                "executionInfo": {
                    "elapsed": 547,
                    "status": "ok",
                    "timestamp": 1650396260154,
                    "user": {
                        "displayName": "River Kelly",
                        "userId": "17493515786731327813"
                    },
                    "user_tz": 360
                },
                "id": "35Z8vSMqR3r1",
                "outputId": "9a6edbd9-3fe3-4712-db2c-e2eb8674335d"
            },
            "outputs": [],
            "source": [
                "plt.scatter(D[:,0], D[:,1], c=labels)\n",
                "plt.xlabel(\"X1\")\n",
                "plt.ylabel(\"X2\")\n",
                "plt.title('2d clusters')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 265
                },
                "executionInfo": {
                    "elapsed": 322,
                    "status": "ok",
                    "timestamp": 1650396571848,
                    "user": {
                        "displayName": "River Kelly",
                        "userId": "17493515786731327813"
                    },
                    "user_tz": 360
                },
                "id": "6MeLpmiLR4QJ",
                "outputId": "3bcb252e-49bf-4ffe-ab30-13b89c977505"
            },
            "outputs": [],
            "source": [
                "kMeanClustering(D, 3, 0.5)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "6XvVGTToV_Df"
            },
            "source": [
                "## 2. (10 points) DBSCAN Clustering Algorithm"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "5tRZu6o_V_Dg"
            },
            "source": [
                "A function that implements the DBSCAN clustering\n",
                "algorithm. The function should take a data matrix and the parameters\n",
                "*minpts* and $\\epsilon$, as input, and return the clusters found using\n",
                "DBSCAN, and for each data point a label of core, border, or noise point."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "OXsSEouNV_Dg"
            },
            "source": [
                "## 3. (Extra Credit - 5 points) Precision of Clustering"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "LsQ8cy3GV_Dg"
            },
            "source": [
                "A function that computes the precision of a\n",
                "clustering. The function should take a list of true cluster labels and a\n",
                "list of the cluster labels returned by some clustering algorithm, and return\n",
                "the precision of the clustering."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "CFmzM0OzV_Dh"
            },
            "source": [
                "# Part 3: Analyze your data"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "2vonj-1lV_Dh"
            },
            "source": [
                "## 1. (4 points)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "b9QYVgJVV_Dh"
            },
            "source": [
                "Use sklearn's PCA implementation to linearly transform the\n",
                "data to two dimensions. Create a scatter plot of the data, with the $x$-axis\n",
                "corresponding to coordinates of the data along the first principal\n",
                "component, and the $y$-axis corresponding to coordinates of the data along\n",
                "the second principal component. Does it look like there are clusters in\n",
                "these two dimensions? If so, how many would you say there are?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "-xUdq5zVV_Di"
            },
            "source": [
                "## 2. (3 points)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "oEGT5QQ0V_Di"
            },
            "source": [
                "Use sklearn's PCA implementation to linearly transform the\n",
                "data, without specifying the number of components to use. Create a plot with\n",
                "$r$, the number of components (i.e., dimensionality), on the $x$-axis, and\n",
                "$f(r)$, the fraction of total variance captured in the first $r$ principal\n",
                "components, on the $y$-axis. Based on this plot, choose a number of\n",
                "principal components to reduce the dimensionality of the data. Report how\n",
                "many principal components will be used as well as the faction of total\n",
                "variance captured using this many components."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "q68KBWEEV_Dj"
            },
            "source": [
                "## 3. (5 points)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "ezOzxo3rV_Dj"
            },
            "source": [
                "For both the original and the reduced-dimensionality data\n",
                "obtained using PCA in question 3, do the\n",
                "following: Experiment with a range of values for the number of clusters,\n",
                "$k$, that you pass as input to the $k$-means function, to find clusters in\n",
                "the chosen data set. Use at least 5 different values of $k$. For each value\n",
                "of $k$, report the value of the objective function for that choice of $k$."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "DvvehwZLV_Dj"
            },
            "source": [
                "## 4. (5 points)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "lhKb3cShV_Dk"
            },
            "source": [
                "For both the original and the reduced-dimensionality data\n",
                "obtained using PCA in question 3, do the following:\n",
                "Experiment with a range of values for the *minpts* and $\\epsilon$ input\n",
                "parameters to the DBSCAN function to find clusters in the chosen data set.\n",
                "First, keep $\\epsilon$ fixed and try out a range of different values for\n",
                "*minpts*. Then keep *minpts* fixed, and try a range of values for\n",
                "$\\epsilon$. Use at least 5 values of $\\epsilon$ and at least 5 values of\n",
                "*minpts*. Report the number of clusters found for each (*minpts*,\n",
                "$\\epsilon$) pair tested."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "EimFxiorV_Dk"
            },
            "source": [
                "## 5. (Extra credit - 3 points)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "o5sXosGLV_Dk"
            },
            "source": [
                "Create a plot of clustering precision for\n",
                "each value of $k$ used in question 3, each value of\n",
                "$\\epsilon$ used in question 3, and each value of\n",
                "*minpts* used in question 3, for both the original\n",
                "and reduced-dimensionality data."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "yjCVTa__V_Dk"
            },
            "source": [
                "# Tips and Acknowledgements"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "kQjklTosV_Dl"
            },
            "source": [
                "Make sure to submit your answer as a PDF on Gradscope and Brightspace. Make sure\n",
                "to show your work. Include any code snippets you used to generate an answer,\n",
                "using comments in the code to clearly indicate which problem corresponds to\n",
                "which code."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "PHDraLpHV_Dl"
            },
            "source": [
                "**Acknowledgements**: Project adapted from assignments of Veronika Strnadova-Neeley."
            ]
        }
    ],
    "metadata": {
        "colab": {
            "collapsed_sections": [],
            "name": "project-03.ipynb",
            "provenance": []
        },
        "interpreter": {
            "hash": "00684e69adb826331b650ae4febdd10eef3cfe83175fe79e1892054a99ff83e7"
        },
        "kernelspec": {
            "display_name": "Python 3.8.12 ('csci347')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.12"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
