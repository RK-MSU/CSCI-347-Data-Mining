# Categorical Data

## Review

**Estimated variance** $X_j$: $$\hat{\sigma}^2_j = \frac{1}{n-1}\sum_{i=1}^n (x_{ij} - \hat{\mu}_j)^2$$

**Estimated standard deviation of** $X_j$: $$\hat{\sigma}_j = \sqrt{\hat{\sigma}_j^2}$$

**Covariance of $X_i$ and $X_j$**: $$\hat{\sigma}_{ij} = \frac{1}{n-1}\sum_{k=1}^n (x_{ki} - \hat{\mu_i})(x_{kj} - \hat{\mu_j})$$

**Personâ€™s correlation coefficient of $X_i$ and $X_j$**: $$\hat{\rho}_{ij} = \frac{\hat{\sigma}_{ij}}{\hat{\sigma}_i \hat{\sigma}_j}$$

**Euclidean Distance**:

$L_2$ norm: $\|x_i - x_j \|_2 = \sqrt{ \sum_{k=1}^{m} (x_{ik} - x_{jk} )^2 }$ where $x_i$ and $x_j$ are vectors, and there are $m$ dimensions.

$$D = \begin{matrix}
 & X_1 & X_2  & X_3 \\
x_1 & 0.2 & 23 & 5.7 \\
x_2 & 0.4 & 1& 5.4 \\
x_3 & 1.8 & 0.5  & 5.2 \\
x_4 & 5.6 & 50 & 5.1 \\
x_5 & -0.5 & 34  & 5.3 \\
x_6 & 0.4 & 19 & 5.4 \\
x_7 & 1.1 & 11  & 5.5 \\
\end{matrix}$$

$$\begin{align*}
    \|x_1 - x_2 \|_2 &= \sqrt{ \sum_{k=1}^{3} (x_{1k} - x_{2k} )^2 } \\
    &= \sqrt{ (x_{11} - x_{21} )^2 + (x_{12} - x_{22} )^2 + (x_{13} - x_{23} )^2} \\
    &= \sqrt{ (0.2 - 0.4 )^2 + (23 - 1)^2 + (5.7 - 5.4 )^2} \\
    &= \sqrt{ (-0.2 )^2 + (22)^2 + (0.3 )^2} \\
    &= 22.0
\end{align*}$$

## One-Hot Encoding

$$D = \begin{matrix}
    & X_1 & X_2  & X_3 & X_4 \\
x_1 & 0.2 & 23   & 5.7 & A \\
x_2 & 0.4 &  1.  & 5.4 & B \\
x_3 & 1.8 & 0.5  & 5.2 & C \\
x_4 & 5.6 & 50   & 5.1 & A \\
x_5 & -0.5 & 34  & 5.3 & B \\
x_6 & 0.4 & 19 & 5.4   & C \\
x_7 & 1.1 & 11  & 5.5 & C \\
\end{matrix}$$

Converts to:

$$D = \begin{matrix}
    & X_1 & X_2  & X_3 & X_{4A} & X_{4B} & X_{4C} \\
x_1 & 0.2 & 23   & 5.7 & 1 & 0 & 0 \\
x_2 & 0.4 &  1.  & 5.4 & 0 & 1 & 0 \\
x_3 & 1.8 & 0.5  & 5.2 & 0 & 0 & 1 \\
x_4 & 5.6 & 50   & 5.1 & 1 & 0 & 0 \\
x_5 & -0.5 & 34  & 5.3 & 0 & 1 & 0 \\
x_6 & 0.4 & 19 & 5.4   & 0 & 0 & 1 \\
x_7 & 1.1 & 11  & 5.5 &  0 & 0 & 1 \\
\end{matrix}$$

For one-hot encoded data, the number of matching categorical values is the dot product of their vectors

$$D = \begin{matrix}
    & X_4 \\
x_1 & A   \\
x_2 & B   \\
x_3 & C   \\
x_4 & A   \\
x_5 & B   \\
x_6 & C   \\
x_7 & C   \\
\end{matrix}$$

$$D = \begin{matrix}
    & X_{4A} & X_{4B} & X_{4C} \\
x_1 & 1 & 0 & 0 \\
x_2 &  0 & 1 & 0   \\
x_3 &  0 & 0 & 1   \\
x_4 & 1 & 0 & 0   \\
x_5 &  0 & 1 & 0  \\
x_6 & 0 & 0 & 1  \\
x_7 & 0 & 0 & 1 \\
\end{matrix}$$

$x_1 \cdot x_2 = 1*0 + 0*1 + 0*0 = 0$

### Hamming Distance

The number of mismatches between two vectors

Recall $XOR \oplus$

$$\begin{matrix}
  a & b & a \oplus b \\
  0 & 0 & 0 \\
  0 & 1 & 1 \\
  1 & 0 & 1 \\
  1 & 1 & 0 \\
\end{matrix}$$


Given: 

$$D = \begin{matrix}
    & X_1 & X_2 \\
x_1 & A   & H  \\
x_2 & B   & L  \\
x_3 & C   & L  \\
x_4 & A   & L  \\
x_5 & B   & H  \\
x_6 & C   & L  \\
x_7 & C   & H  \\
\end{matrix}$$

$$D = \begin{matrix}
    & X_{1A} & X_{1B} & X_{1C} & X_{2A} & X_{2B} \\
x_1 & 1 & 0 & 0 & 1 & 0 \\
x_2 & 0 & 1 & 0 & 0 & 1  \\
x_3 & 0 & 0 & 1 & 0 & 1  \\
x_4 & 1 & 0 & 0 & 0 & 1  \\
x_5 & 0 & 1 & 0 & 1 & 0  \\
x_6 & 0 & 0 & 1 & 0 & 1  \\
x_7 & 0 & 0 & 1 & 1 & 0  \\
\end{matrix}$$

$$\begin{align*}
    \delta_H(x_1, x_2) &= sum(x1 \oplus x2) \\
    &= (1 \oplus 0) + (0 \oplus 1) + (0 \oplus 0) + (1 \oplus 0) + (0 \oplus 1) \\
    &= 1 + 1 + 0 + 1 + 1 = 4 \\
    &= 4
\end{align*}$$

## Jaccard Similarity

The size of the intersection over the size of the union

$$J(x_i, x_j) = \frac{| x_i \cap x_j |}{| x_i \cup x_j|} =  \frac{ sum(x_i \land x_j) }{ sum( x_i \lor x_j)}$$

Given:

$$D = \begin{matrix}
    & X_{1} & X_{2} & X_{3} & X_{4} & X_{5} \\
x_1 & 1 & 0 & 0 & 1 & 0 \\
x_2 & 0 & 1 & 0 & 0 & 1  \\
x_3 & 0 & 0 & 1 & 0 & 1  \\
x_4 & 1 & 0 & 0 & 0 & 1  \\
x_5 & 0 & 1 & 0 & 1 & 0  \\
x_6 & 0 & 0 & 1 & 0 & 1  \\
x_7 & 0 & 0 & 1 & 1 & 0  \\
\end{matrix}$$

Then:

$$\begin{align*}
    J(x_1, x_2) &= \frac{sum(x1 \land x2)}{sum(x1 \lor x2)} \\
    &= \frac{(1 \land 0) + (0 \land 1) + (0 \land 0) + (1 \land 0) + (0 \land 1)}{(1 \lor 0) + (0 \lor 1) + (0 \lor 0) + (1 \lor 0) + (0 \lor 1)} \\
    &= \frac{0 + 0 + 0 + 0 + 0}{ 1 + 1 + 0 + 1 + 1} \\
    &= 0
\end{align*}$$

